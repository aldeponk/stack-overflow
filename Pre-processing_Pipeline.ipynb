{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "from langdetect import detect\n",
    "import re\n",
    "import spacy\n",
    "from spacy.symbols import ORTH, NORM\n",
    "from langdetect import detect_langs\n",
    "import langid\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset/dataset.csv',sep=\",\",nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logIt(caller,s):\n",
    "    dt = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    show_log = f'{caller: <25} ' + s\n",
    "    print(dt + ' - ' + show_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   Only keep english records \n",
    "Input:  Dataset\n",
    "Output: only english records dataset  \n",
    "\n",
    "Traitements appliqués : \n",
    "- create new feature with language\n",
    "- remove all records where title language is not english\n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def removeNonEngRecords(df,caller):\n",
    "    \n",
    "    language = []\n",
    "    for title in data['title']:\n",
    "        lang, log_prob = langid.classify(title)\n",
    "        #print(lang)\n",
    "        language.append(lang)\n",
    "    \n",
    "    df['lang'] = language\n",
    "    \n",
    "    df.drop(df[df.lang != \"en\"].index, inplace=True)\n",
    "    df.drop(['lang'], inplace=True, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   Merge des colonnes title et body\n",
    "Input:  Dataset\n",
    "Output: dataset with body feature merged with title   \n",
    "\n",
    "Traitements appliqués : \n",
    "- remove \n",
    "- create new column with extracted text\n",
    "- \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def mergeTitle(df, caller):\n",
    "    \n",
    "    \n",
    "    df['final_body'] = df[\"title\"] + ' ' + df[\"body\"]\n",
    "    df.drop(['body'], inplace=True, axis=1)\n",
    "    df.columns = ['title','tags','body']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   Extract text from questions body \n",
    "Input:  Dataset\n",
    "Output: dataset with body feature without tags   \n",
    "\n",
    "Traitements appliqués : \n",
    "- extract text from tags contents\n",
    "- create new column with extracted text\n",
    "- \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def extractText(df, caller):\n",
    "    bodies = []\n",
    "    for body in data['body']: \n",
    "        #print(body)\n",
    "        soup = BeautifulSoup(body)    \n",
    "        tags = [tag.name for tag in soup.find_all()]\n",
    "        #print(tags)\n",
    "        #loop tag to remove code and href tags\n",
    "        if \"pre\" in tags:\n",
    "            soup.pre.decompose()\n",
    "        if \"code\" in tags:\n",
    "            try:\n",
    "                soup.code.decompose()\n",
    "            except:\n",
    "                pass\n",
    "        if \"a\" in tags:\n",
    "            try:\n",
    "                soup.a.decompose()\n",
    "            except:\n",
    "                pass\n",
    "        bodies.append(soup.get_text())\n",
    "        \n",
    "    df['final_body'] = bodies\n",
    "    df.drop(['body'], inplace=True, axis=1)\n",
    "    df.columns = ['title', 'tags','body']\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   Remove \\n \n",
    "Input:  Dataset\n",
    "Output: dataset with body feature without useless characters   \n",
    "\n",
    "Traitements appliqués : \n",
    "- remove \n",
    "- create new column with extracted text\n",
    "- \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def removeLineFeed(df, caller):\n",
    "    bodies = []\n",
    "    for body in data['body']: \n",
    "        final_body = body.replace('\\n',' ')\n",
    "        bodies.append(final_body)\n",
    "\n",
    "    df['final_body'] = bodies\n",
    "    df.drop(['body'], inplace=True, axis=1)\n",
    "    df.columns = ['title', 'tags','body']\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   Remove useless characters \n",
    "Input:  Dataset\n",
    "Output: dataset with body feature without useless characters   \n",
    "\n",
    "Traitements appliqués : \n",
    "- remove \n",
    "- create new column with extracted text\n",
    "- \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def removeUselessChars(df, caller):\n",
    "    bodies = []\n",
    "    \n",
    "    pattern = re.compile('[^A-Za-z +]')\n",
    "\n",
    "    for body in data['body']: \n",
    "        final_body = pattern.sub(' ',body)        \n",
    "        final_body = re.sub(pattern, ' ', body)\n",
    "        bodies.append(final_body)\n",
    "\n",
    "    df['final_body'] = bodies\n",
    "    df.drop(['body'], inplace=True, axis=1)\n",
    "    df.columns = ['title', 'tags','body']\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   toLowerCase \n",
    "Input:  Dataset\n",
    "Output: dataset with body feature to lower case   \n",
    "\n",
    "Traitements appliqués : \n",
    "- remove \n",
    "- create new column with extracted text\n",
    "- \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def toLowerCase(df, caller):\n",
    "    bodies = []\n",
    "    \n",
    "    for body in data['body']: \n",
    "        final_body = body.lower()\n",
    "        bodies.append(final_body)\n",
    "\n",
    "    df['final_body'] = bodies\n",
    "    df.drop(['body'], inplace=True, axis=1)\n",
    "    df.columns = ['title', 'tags','body']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   unitary stopwords removal function \n",
    "Input:  text\n",
    "Output: text without stopwords   \n",
    "\n",
    "Traitements appliqués : \n",
    "- stop words removal \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    #print(text)\n",
    "    word_tokens = word_tokenize(text) \n",
    "\n",
    "    filtered_text = ' '.join(w for w in word_tokens if not w in stop_words)\n",
    "   \n",
    "    return filtered_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   Suppress stopwords, english corpus \n",
    "Input:  Dataset\n",
    "Output: dataset with body feature without stopwords   \n",
    "\n",
    "Traitements appliqués : \n",
    "- remove \n",
    "- create new column with extracted text\n",
    "- \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def removeStopwords(df, caller):\n",
    "    bodies = []\n",
    "\n",
    "    df['no_stop_words'] = df.body.apply(remove_stopwords)    \n",
    "\n",
    "    df.drop(['body'], inplace=True, axis=1)\n",
    "    df.columns = ['title', 'tags','body']\n",
    "    \n",
    "    return df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   unitary Lemmatization & Stemming\n",
    "Input:  text\n",
    "Output: lemmatized and stemmed text   \n",
    "\n",
    "Traitements appliqués : \n",
    "- lemmatization \n",
    "- stemming\n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def lemmatize_stem_text(text):\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    stemmer = EnglishStemmer()\n",
    "    stemmed = ' '.join(stemmer.stem(WordNetLemmatizer().lemmatize(w, pos='v')) for w in w_tokenizer.tokenize(text))\n",
    "    #print(stemmed)\n",
    "    #return [stemmer.stem(WordNetLemmatizer().lemmatize(w, pos='v')) for w in w_tokenizer.tokenize(text)]\n",
    "    return stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   Lemmatization & Stemming\n",
    "Input:  Dataset\n",
    "Output: dataset with body feature normalized by lemmatization and stemming  \n",
    "\n",
    "Traitements appliqués : \n",
    "- remove \n",
    "- create new column with extracted text\n",
    "- \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def normalize(df, caller):\n",
    "    bodies = []\n",
    "\n",
    "    df['text_lemmatized'] = df.body.apply(lemmatize_stem_text)    \n",
    "    \n",
    "    df.drop(['body'], inplace=True, axis=1)\n",
    "    df.columns = ['title', 'tags','body']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   remove Nan rows with empty body\n",
    "Input:  Dataset\n",
    "Output: dataset without Nan body feature   \n",
    "\n",
    "Traitements appliqués : \n",
    "- valuate to empty string \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def removeNanBodies(df, caller):\n",
    "\n",
    "    df['length'] = df.body.str.len()    \n",
    "    df.drop(df[df.length == 0].index, inplace=True)\n",
    "    df.drop(['length'], inplace=True, axis=1)       \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   remove Nan rows with empty tag\n",
    "Input:  Dataset\n",
    "Output: dataset without Nan tags feature   \n",
    "\n",
    "Traitements appliqués : \n",
    "- valuate to empty string \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def removeNanTags(df, caller):\n",
    "\n",
    "    df['length'] = df.tags.str.len()    \n",
    "    df.drop(df[df.length == 0].index, inplace=True)\n",
    "    df.drop(['length'], inplace=True, axis=1)       \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   normalize tags column \n",
    "Input:  Dataset\n",
    "Output: dataset with tags feature ready to be used   \n",
    "\n",
    "Traitements appliqués : \n",
    "- remove \n",
    "- create new column with extracted text\n",
    "- \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def setupTags(df, caller):\n",
    "    tags = []\n",
    "    \n",
    "    for tag in data['tags']: \n",
    "        final_tags = tag.replace('<',',')\n",
    "        final_tags = final_tags.replace('>','')\n",
    "        final_tags = final_tags[1:]\n",
    "        \n",
    "        tags.append(final_tags)\n",
    "\n",
    "    df['final_tags'] = tags\n",
    "    df.drop(['tags'], inplace=True, axis=1)\n",
    "    df.columns = ['title', 'body', 'tags']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "Desc:   normalize tags column \n",
    "Input:  Dataset\n",
    "Output: dataset with tags feature ready to be used   \n",
    "\n",
    "Traitements appliqués : \n",
    "- remove \n",
    "- create new column with extracted text\n",
    "- \n",
    "-----------------------------------------------------------------\n",
    "\"\"\"\n",
    "def dropDuplicates(df, caller):\n",
    "    bodies = []\n",
    "    for body in data['body']: \n",
    "        t = set(body.split())\n",
    "        final_body = ' '.join(t)        \n",
    "        \n",
    "        bodies.append(final_body)\n",
    "\n",
    "    df['final_tags'] = bodies\n",
    "    df.drop(['body'], inplace=True, axis=1)\n",
    "    df.columns = ['title', 'tags','body']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "Desc:   Pipeline de pre-processing du dataset\n",
    "Input:  Dataframe\n",
    "Output: pre-processed dataframe \n",
    "\n",
    "Traitements appliqués : \n",
    "- \n",
    "\n",
    "-----------------------------------------------------------------\n",
    "\"\"\"  \n",
    "\n",
    "def preprocessing_pipeline(data):\n",
    "\n",
    "    logIt('[Remove non eng. rec]', 'Starting')\n",
    "    data = removeNonEngRecords(data,'test')\n",
    "    logIt('[Remove non eng. rec]', 'Completed')\n",
    "\n",
    "    #logIt('[Merge Title]', 'Starting')\n",
    "    #data = mergeTitle(data,'test')\n",
    "    #logIt('[Merge Title]', 'Completed')\n",
    "    \n",
    "    logIt('[Extract text from HTML]', 'Starting')\n",
    "    data = extractText(data, 'test')\n",
    "    logIt('[Extract text from HTML]', 'Completed')\n",
    "    \n",
    "    logIt('[Remove line feed]', 'Starting')\n",
    "    data = removeLineFeed(data,'test')\n",
    "    logIt('[Remove line feed]', 'Completed')\n",
    "    \n",
    "    logIt('[Remove useless chars]', 'Starting')\n",
    "    data = removeUselessChars(data,'test')\n",
    "    logIt('[Remove useless chars]', 'Completed')\n",
    "    \n",
    "    logIt('[Lower]', 'Starting')\n",
    "    data = toLowerCase(data,'test')\n",
    "    logIt('[Lower]', 'Completed')\n",
    "    \n",
    "    logIt('[Remove Stop words]', 'Starting')\n",
    "    data = removeStopwords(data,'test')\n",
    "    logIt('[Remove Stop words]', 'Completed')\n",
    "    \n",
    "    logIt('[Lemmatization]', 'Starting')\n",
    "    data = normalize(data,'test')\n",
    "    logIt('[Lemmatization]', 'Completed')\n",
    "    \n",
    "    logIt('[Remove nan bodies]', 'Starting')\n",
    "    data = removeNanBodies(data,'test')\n",
    "    logIt('[Remove nan bodies]', 'Completed')\n",
    "\n",
    "    logIt('[Remove nan tags]', 'Starting')\n",
    "    data = removeNanTags(data,'test')\n",
    "    logIt('[Remove nan tags]', 'Completed')\n",
    "    \n",
    "    logIt('[Setup Tags]', 'Starting')\n",
    "    data = setupTags(data,'test')\n",
    "    logIt('[Setup Tags]', 'Completed')\n",
    "    \n",
    "    #logIt('[Drop duplacate tokens]', 'Starting')\n",
    "    #data = dropDuplicates(data,'test')\n",
    "    #logIt('[Drop duplacate tokens]', 'Completed')\n",
    "\n",
    "    logIt('[Drop title]', 'Starting')\n",
    "    data.drop(['title'], inplace=True, axis=1)\n",
    "    logIt('[Drop title]', 'Completed')\n",
    "    \n",
    "    logIt('[Save preprocessed dataset]', 'Starting')\n",
    "    data.to_csv('./dataset/preprocessed_dataset.csv', index=False)\n",
    "    logIt('[Save preprocessed dataset]', 'Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-01 17:54:22 - [Remove non eng. rec]     Starting\n",
      "2021-05-01 17:54:35 - [Remove non eng. rec]     Completed\n",
      "2021-05-01 17:54:35 - [Extract text from HTML]  Starting\n",
      "2021-05-01 17:54:38 - [Extract text from HTML]  Completed\n",
      "2021-05-01 17:54:38 - [Remove line feed]        Starting\n",
      "2021-05-01 17:54:38 - [Remove line feed]        Completed\n",
      "2021-05-01 17:54:38 - [Remove useless chars]    Starting\n",
      "2021-05-01 17:54:38 - [Remove useless chars]    Completed\n",
      "2021-05-01 17:54:38 - [Lower]                   Starting\n",
      "2021-05-01 17:54:38 - [Lower]                   Completed\n",
      "2021-05-01 17:54:38 - [Remove Stop words]       Starting\n",
      "2021-05-01 17:54:42 - [Remove Stop words]       Completed\n",
      "2021-05-01 17:54:42 - [Lemmatization]           Starting\n",
      "2021-05-01 17:54:49 - [Lemmatization]           Completed\n",
      "2021-05-01 17:54:49 - [Remove nan bodies]       Starting\n",
      "2021-05-01 17:54:49 - [Remove nan bodies]       Completed\n",
      "2021-05-01 17:54:49 - [Remove nan tags]         Starting\n",
      "2021-05-01 17:54:49 - [Remove nan tags]         Completed\n",
      "2021-05-01 17:54:49 - [Setup Tags]              Starting\n",
      "2021-05-01 17:54:49 - [Setup Tags]              Completed\n",
      "2021-05-01 17:54:49 - [Drop title]              Starting\n",
      "2021-05-01 17:54:49 - [Drop title]              Completed\n",
      "2021-05-01 17:54:49 - [Save preprocessed dataset] Starting\n",
      "2021-05-01 17:54:49 - [Save preprocessed dataset] Completed\n"
     ]
    }
   ],
   "source": [
    "preprocessing_pipeline(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
