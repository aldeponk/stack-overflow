{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet de modélisation supervisée des fonctionnalités de prédiction des tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "from langdetect import detect\n",
    "import re\n",
    "import spacy\n",
    "from spacy.symbols import ORTH, NORM\n",
    "from langdetect import detect_langs\n",
    "import langid\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset/preprocessed_dataset.csv',sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Démarche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de modélisation supervisée dans le cadre des posts StackOverflow consiste à classifier des questions à partir de leur corpus lemmatizé et labelisé par les tags.\n",
    "\n",
    "Nous sommes face à un problème de classification multi-label.\n",
    "\n",
    "Nous choisissons de mettre en place un modèle à base de CNN, le jeu de données en entrée fait l'objet d'une vectorisation par TF_IDF.\n",
    "\n",
    "Nous effectuons une cross-validation pour obtenir la visibilité sur l'accuracy et la stabilité du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constitution des datasets de training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['!', '?', ',', '.']\n",
    "intents_file = codecs.open('intents.json', 'r', encoding='utf-8').read()\n",
    "\n",
    "intents = json.loads(intents_file)\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        words.extend(word)\n",
    "        #add documents in the corpus\n",
    "        documents.append((word, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "#print(documents)\n",
    "# lemmaztize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "#print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "#print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "#print (len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))\n",
    "\n",
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for word in words:\n",
    "        bag.append(1) if word in pattern_words else bag.append(0)\n",
    "        \n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    result = \"\"\n",
    "    #print(type(text))\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "             result = result + ' ' + token\n",
    "    return result\n",
    "\n",
    "processed_body = data['body'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000, use_idf=True)\n",
    "vectorizer = vectorizer.fit(data['body'])\n",
    "tf_len = len(vectorizer.vocabulary_)\n",
    "X = vectorizer.transform(processed_body).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tags = data['tags'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = vectorizer.transform(processed_tags).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "    # Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training avec Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, y):\n",
    "\tresults = list()\n",
    "\tn_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# enumerate folds\n",
    "\tfor train_ix, test_ix in cv.split(X):\n",
    "\t\t# prepare data\n",
    "\t\tX_train, X_test = X[train_ix], X[test_ix]\n",
    "\t\ty_train, y_test = y[train_ix], y[test_ix]\n",
    "\t\t# define model\n",
    "\t\tmodel = get_model(n_inputs, n_outputs)\n",
    "\t\t# fit model\n",
    "\t\tmodel.fit(X_train, y_train, verbose=0, epochs=100)\n",
    "\t\t# make a prediction on the test set\n",
    "\t\tyhat = model.predict(X_test)\n",
    "\t\t# round probabilities to class labels\n",
    "\t\tyhat = yhat.round()\n",
    "\t\t# calculate accuracy\n",
    "\t\tacc = accuracy_score(y_test, yhat)\n",
    "\t\t# store result\n",
    "\t\tprint('>%.3f' % acc)\n",
    "\t\tresults.append(acc)\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(X, y)\n",
    "# summarize performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(results), std(results)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
